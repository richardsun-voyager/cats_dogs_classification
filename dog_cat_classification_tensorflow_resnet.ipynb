{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构\n",
    "\n",
    "参考了tensorflow官方网站：https://github.com/tensorflow/models/blob/master/official/resnet/resnet.py 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "TOWER_NAME = 'tower'\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "           # with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define dog cat classification model\n",
    "class imageClassification:\n",
    "    '''Define a basic model for image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, image_holder, label_holder, keep_prob, batch_size=32, num_class=1, is_training=True):\n",
    "        self.image_holder = image_holder\n",
    "        self.label_holder = label_holder\n",
    "        self.num_class = num_class\n",
    "        self.is_training = is_training\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimizer\n",
    "        self.accuracy\n",
    "        self.correct_num\n",
    "        print('Initializing Image Classification Model!') \n",
    "    \n",
    "    @define_scope\n",
    "    def sigmoid(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        logits = self.build_network\n",
    "        return tf.nn.sigmoid(logits)\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        sigmoid = self.sigmoid\n",
    "        predictions = sigmoid > 0.5\n",
    "        return tf.cast(predictions, tf.int32)\n",
    "    \n",
    "    @define_scope\n",
    "    def cost(self):\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        labels = tf.cast(self.label_holder, tf.float32)\n",
    "        #Cross entropy\n",
    "        #cross_entropy = -tf.reduce_mean(labels*\n",
    "                                       #tf.log(self.prediction), name='cross_entropy')\n",
    "        clipped_value = tf.clip_by_value(self.sigmoid, 1e-10, 0.999)\n",
    "        cross_entropy = -tf.reduce_mean(labels*tf.log(clipped_value) + (1-labels)*tf.log(1-clipped_value))\n",
    "        \n",
    "        #Regularization\n",
    "        l2_loss = cross_entropy\n",
    "        for i in range(len(self.weights)):\n",
    "                l2_loss += tf.nn.l2_loss(self.weights[i])\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    @define_scope\n",
    "    def optimizer(self):\n",
    "        '''Define cross entropy loss function and regularization'''      \n",
    "        l2_loss = self.cost\n",
    "        #Decaying Learning Rate\n",
    "        cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.0001\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 2000, 0.96, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(l2_loss, global_step=cur_step)\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        '''Calculate accuracy for each epoch of training'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        correct_ones = tf.reduce_sum(tf.cast(correct_prediction, tf.int32))\n",
    "        return correct_ones\n",
    "    \n",
    "    def _conv_layer(self, input_tensor, shape, wd, scope, is_pooling=True): \n",
    "        '''\n",
    "        Create a layer of convolutional neural network\n",
    "        Args:\n",
    "        input_tensor:input, [batch_size, height, width, channel]\n",
    "        shape: shape of weights, [filter_width, filter_height, filter_channel, output_channel]\n",
    "        wd: decaying weight\n",
    "        scope: variable scope\n",
    "        '''\n",
    "        #Get the output dim\n",
    "        output_dim = shape[-1]\n",
    "        #Create variables\n",
    "        kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=shape,\n",
    "                                                 wd=wd)\n",
    "        #Compute convolution\n",
    "        conv = tf.nn.conv2d(input_tensor, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = self._variable_on_cpu('biases', [output_dim], tf.constant_initializer(0.0))\n",
    "        z = tf.nn.bias_add(conv, biases)\n",
    "        #Batch normalization\n",
    "        bn = self._batch_normalization(z)\n",
    "        activation = tf.nn.relu(bn, name=scope.name)\n",
    "        \n",
    "        self._activation_summary(activation)\n",
    "        if is_pooling:\n",
    "            pool = tf.nn.max_pool(activation, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "        else:\n",
    "            pool = activation\n",
    "        return pool\n",
    "    \n",
    "    def _bottle_block(self, input_data):\n",
    "        '''\n",
    "        Build a residual block\n",
    "        Args:\n",
    "        input_data: tensor, batch_size*height*width*channel\n",
    "        '''\n",
    "        final_filter_size = 64\n",
    "        with tf.variable_scope('preprocess_input') as scope:\n",
    "            shape = [1, 1, 3, 64]\n",
    "            wd = 0\n",
    "            #batch*64*64*64\n",
    "            #Use 1*1 filters to downsample input\n",
    "            input_data = self._conv_layer(input_data, shape, wd, scope, False)\n",
    "        with tf.variable_scope('bottle_block') as scope:\n",
    "            output = self._create_block_(input_data)\n",
    "        \n",
    "        return output\n",
    "            \n",
    "    def _create_block_(self, input_data):\n",
    "        '''Create identity block'''\n",
    "        shortcut = input_data\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 64, 128]\n",
    "            #batch*64*64*128\n",
    "            input_data = self._conv_layer(input_data, shape, wd, scope, False)\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 128, 128]\n",
    "            #batch*64*64*128\n",
    "            input_data = self._conv_layer(input_data, shape, wd, scope, False)\n",
    "        #Conv3 without BN\n",
    "        with tf.variable_scope('conv3') as cope:\n",
    "            #Create variables\n",
    "            shape = [3, 3, 128, 64]\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=shape,\n",
    "                                                 wd=0)\n",
    "            #Compute convolution\n",
    "            conv = tf.nn.conv2d(input_data, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "            #batch_size*64*64*64\n",
    "            input_data = tf.nn.bias_add(conv, biases)\n",
    "        return shortcut + input_data\n",
    "        \n",
    "    \n",
    "    def _fully_layer(self, input_data, out_dim, scope, wd=0.004):\n",
    "        '''\n",
    "        Create a fully connected layer with specific parameters\n",
    "        Args:\n",
    "        input_data:input tensor, batch_size*width*height*channels\n",
    "        out_dim: output dimension\n",
    "        wd: punishment for l2 regularization\n",
    "        '''\n",
    "        reshape_tensor = tf.reshape(input_data, [self.batch_size, -1], name='reshape_tensor')\n",
    "        in_dim = reshape_tensor.get_shape()[1].value\n",
    "        weights = self._variable_with_weight_decay('weights', shape=[in_dim, out_dim], wd=0.004)\n",
    "        biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
    "        local = tf.nn.relu(tf.matmul(reshape_tensor, weights) + biases, name=scope.name)\n",
    "        self._activation_summary(local)\n",
    "        return local\n",
    "    \n",
    "    def _batch_normalization(self, z):\n",
    "        '''\n",
    "        Calculate batch nomralization of the input data\n",
    "        Args:\n",
    "        z: weighted linear sums of previous neurons\n",
    "        '''\n",
    "        decay = 0.997\n",
    "        epsilon = 0.001\n",
    "        #Scale mean value\n",
    "        bias_shape = z.get_shape()[-1]\n",
    "        gamma = tf.get_variable(name='gamma', initializer=tf.ones(bias_shape))\n",
    "        beta = tf.get_variable(name='beta', initializer=tf.zeros(bias_shape))\n",
    "        moving_mean = tf.get_variable(name='moving_mean', initializer=tf.zeros(bias_shape), trainable=False)\n",
    "        moving_var = tf.get_variable(name='moving_var', initializer=tf.ones(bias_shape), trainable=False)\n",
    "        axis = list(range(len(z.get_shape()) - 1))\n",
    "        #If training\n",
    "        if self.is_training:\n",
    "            batch_mean, batch_var = tf.nn.moments(z, axis)\n",
    "            train_mean = tf.assign(moving_mean,\n",
    "                                   moving_mean * decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.assign(moving_var,\n",
    "                                  moving_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                bn = tf.nn.batch_normalization(z,\n",
    "                                           batch_mean, batch_var, beta, gamma, epsilon)    \n",
    "        else:\n",
    "            bn = tf.nn.batch_normalization(z, moving_mean, moving_var, beta, gamma, epsilon)\n",
    "        return bn\n",
    "    \n",
    "    def _conv_base(self, input_data):\n",
    "        '''\n",
    "        Set a series of convolutional layers\n",
    "        Args:\n",
    "        input_data:tensor, batch_size*height*width*channel\n",
    "        '''\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            shape = [3, 3, 3, 128]\n",
    "            wd = 0\n",
    "            #32*32*32\n",
    "            pool1 = self._conv_layer(input_data, shape, wd, scope)\n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 128, 128]\n",
    "            #16*16*64\n",
    "            pool2 = self._conv_layer(pool1, shape, wd, scope)\n",
    "        # conv3\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 64, 128]\n",
    "            #8*8*128\n",
    "            #pool3 = self._conv_layer(pool2, shape, wd, scope)\n",
    "        activation = pool2\n",
    "        return activation\n",
    "    \n",
    "    \n",
    "    @define_scope\n",
    "    def build_network(self):\n",
    "        \"\"\"Build the clasification network.\n",
    "        Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "        Returns:\n",
    "        Logits.\n",
    "        \"\"\"\n",
    "        # We instantiate all variables using tf.get_variable() instead of\n",
    "        # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "        # If we only ran this model on a single GPU, we could simplify this function\n",
    "        # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "        #batch_szie*64*64*64\n",
    "        output = self._bottle_block(self.image_holder)\n",
    "        net = tf.identity(output, 'residualnet')\n",
    "        #Flatten the output\n",
    "        with tf.variable_scope('AveragePooling'):\n",
    "            net = tf.nn.avg_pool(net, ksize=[1, 64, 64, 1], strides=[1, 1, 1, 1],\n",
    "                                 padding='VALID', name='pool')\n",
    "        #1X1X64\n",
    "        net = tf.nn.dropout(net, self.keep_prob)\n",
    "        \n",
    "\n",
    "        # local3\n",
    "        #with tf.variable_scope('fully_connected1') as scope:\n",
    "            #local3 = self._fully_layer(net, 384, scope)\n",
    "        #local3_dropout = tf.nn.dropout(local3, self.keep_prob)\n",
    "        # local4\n",
    "        #with tf.variable_scope('fully_connected2') as scope:\n",
    "            #local4 = self._fully_layer(pool3_dropout, 192, scope)\n",
    "            \n",
    "        #local4_dropout = tf.nn.dropout(local4, self.keep_prob)\n",
    "        \n",
    "        # linear layer(WX + b),\n",
    "        # We don't apply softmax here because\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "        #output = local3_dropout\n",
    "        with tf.variable_scope('sigmoid_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [1, 1, 64, self.num_class],\n",
    "                                                  wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_class],\n",
    "                                      tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.conv2d(net, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            sigmoid_linear =  tf.nn.bias_add(conv, biases, name=scope.name)\n",
    "            #remove dimension which is 1\n",
    "            sigmoid_linear = tf.squeeze(sigmoid_linear,[1, 2], name='SpatialSqueeze')\n",
    "            self._activation_summary(sigmoid_linear)\n",
    "            #print(sigmoid_linear)\n",
    "        return sigmoid_linear\n",
    "    \n",
    "    #def _conv_layer(self, )\n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        \"\"\"Helper to create summaries for activations.\n",
    "        Creates a summary that provides a histogram of activations.\n",
    "        Creates a summary that measures the sparsity of activations.\n",
    "        Args:\n",
    "        x: Tensor\n",
    "        Returns:\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "        # session. This helps the clarity of presentation on tensorboard.\n",
    "        tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                          tf.nn.zero_fraction(x))\n",
    "    \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope('weights'):\n",
    "                dtype = tf.float32\n",
    "                var = tf.get_variable(initializer=initializer(shape), dtype=dtype, name=name)\n",
    "            #var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _variable_with_weight_decay(self, name, shape, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        dtype = tf.float32\n",
    "        var = self._variable_on_cpu(name, shape,\n",
    "                               tf.contrib.layers.xavier_initializer())\n",
    "        if wd is not None and wd > 0:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            #tf.add_to_collection('losses', weight_decay)\n",
    "            self.weights.append(weight_decay)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义输入张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Image Classification Model!\n",
      "Initializing Image Classification Model!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    model = imageClassification(image_holder, label_holder, keep_prob, batch_size, num_class =1)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    test_model = imageClassification(image_holder, label_holder, keep_prob, batch_size, 1, False)\n",
    "    #with tf.variable_scope('VGG'):\n",
    "        #model_train = imageClassification(image_holder, label_holder, keep_prob, batch_size=64, num_class =1)\n",
    "    #with tf.variable_scope('VGG', reuse=True):\n",
    "        #model_test = imageClassification(image_holder, label_holder, keep_prob, batch_size=16, num_class =1) \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "path = 'C:/Users/onlooker/Documents/deeplearning_projects/cats_dogs_classification/dataset'\n",
    "training_set = train_datagen.flow_from_directory(path+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(path+'/test_set', shuffle=False,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n",
      "Cost: 0.699562\n",
      "Cost: 0.748133\n",
      "Cost: 0.760389\n",
      "Cost: 0.758636\n",
      "Cost: 0.744452\n",
      "Cost: 0.675127\n",
      "Cost: 0.642052\n",
      "Cost: 0.691931\n",
      "Cost: 0.705634\n",
      "Cost: 0.763947\n",
      "Cost: 0.62636\n",
      "Cost: 0.669657\n",
      "Cost: 0.676574\n",
      "Cost: 0.738372\n",
      "Cost: 0.707732\n",
      "Cost: 0.788785\n",
      "Cost: 0.679465\n",
      "Cost: 0.65383\n",
      "Cost: 0.609788\n",
      "Cost: 0.596649\n",
      "Cost: 0.602925\n",
      "Cost: 0.699726\n",
      "Cost: 0.709959\n",
      "Cost: 0.558101\n",
      "Cost: 0.696731\n",
      "Cost: 0.788858\n",
      "Cost: 0.663498\n",
      "Cost: 0.663941\n",
      "Cost: 0.67317\n",
      "Cost: 0.612972\n",
      "Cost: 0.65691\n",
      "Cost: 0.624331\n",
      "Cost: 0.625723\n",
      "Cost: 0.693337\n",
      "Cost: 0.694747\n",
      "Cost: 0.622706\n",
      "Cost: 0.713837\n",
      "Cost: 0.644558\n",
      "Cost: 0.653393\n",
      "Cost: 0.767651\n",
      "Cost: 0.678513\n",
      "Cost: 0.771622\n",
      "Cost: 0.662824\n",
      "Cost: 0.615796\n",
      "Cost: 0.697555\n",
      "Cost: 0.680838\n",
      "Cost: 0.627471\n",
      "Cost: 0.674279\n",
      "Cost: 0.632614\n",
      "Cost: 0.70593\n",
      "Cost: 0.68007\n",
      "Cost: 0.651491\n",
      "Cost: 0.640878\n",
      "Cost: 0.611938\n",
      "Cost: 0.696133\n",
      "Cost: 0.585758\n",
      "Cost: 0.620741\n",
      "Cost: 0.642872\n",
      "Cost: 0.641953\n",
      "Cost: 0.647034\n",
      "Cost: 0.660182\n",
      "Cost: 0.669964\n",
      "Cost: 0.624047\n",
      "Cost: 0.608454\n",
      "Cost: 0.571602\n",
      "Cost: 0.607914\n",
      "Cost: 0.681462\n",
      "Cost: 0.674897\n",
      "Cost: 0.692108\n",
      "Cost: 0.714777\n",
      "Cost: 0.732382\n",
      "Cost: 0.658627\n",
      "Cost: 0.613399\n",
      "Cost: 0.613248\n",
      "Cost: 0.661722\n",
      "Cost: 0.664649\n",
      "Cost: 0.627559\n",
      "Cost: 0.623188\n",
      "Cost: 0.666664\n",
      "Cost: 0.656381\n",
      "Cost: 0.717328\n",
      "Cost: 0.660388\n",
      "Cost: 0.657577\n",
      "Cost: 0.615302\n",
      "Cost: 0.651826\n",
      "Cost: 0.679178\n",
      "Cost: 0.565479\n",
      "Cost: 0.664804\n",
      "Cost: 0.595917\n",
      "Cost: 0.660113\n",
      "Cost: 0.615338\n",
      "Cost: 0.667757\n",
      "Cost: 0.686953\n",
      "Cost: 0.688221\n",
      "Cost: 0.705055\n",
      "Cost: 0.58772\n",
      "Cost: 0.668534\n",
      "Cost: 0.582942\n",
      "Cost: 0.68022\n",
      "Cost: 0.70106\n",
      "Cost: 0.636649\n",
      "Cost: 0.648275\n",
      "Cost: 0.628965\n",
      "Cost: 0.585476\n",
      "Cost: 0.597154\n",
      "Cost: 0.614722\n",
      "Cost: 0.604953\n",
      "Cost: 0.686969\n",
      "Cost: 0.694583\n",
      "Cost: 0.608893\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_steps = 2001\n",
    "epochs = 10\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    #If there exists models in advance, load them\n",
    "    if os.path.exists(\"./Model/checkpoint\"): # 注意此处路径前添加\"./\" ')\n",
    "        saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 0.5}\n",
    "            _, loss = sess.run([model.optimizer, model.cost], feed_dict=feed_dict)\n",
    "            if step%200 == 0:\n",
    "                #每50次计算准确度\n",
    "                #result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', loss) \n",
    "                #保存模型\n",
    "        saver.save(sess, \"Model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n",
      "Testing Result.....\n",
      "Accuracy: 0.642\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    print('Testing Result.....')  \n",
    "    count = 0\n",
    "    for i in range(40):\n",
    "        batch_data, batch_labels = test_set.next()\n",
    "        #print(batch_data.shape)\n",
    "        #print(i)\n",
    "        batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "        feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 1}\n",
    "        c, a = sess.run([test_model.correct_num, test_model.prediction], feed_dict=feed_dict)\n",
    "        #print(a)\n",
    "        count += c\n",
    "    result = count*1.0/2000\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
