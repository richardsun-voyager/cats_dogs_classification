{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构\n",
    "\n",
    "参看VGG网络结构，适当减少了参数尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "TOWER_NAME = 'tower'\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "           # with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define dog cat classification model\n",
    "class imageClassification:\n",
    "    '''Define a basic model for image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, image_holder, label_holder, keep_prob, batch_size=32, num_class=1, is_training=True):\n",
    "        self.image_holder = image_holder\n",
    "        self.label_holder = label_holder\n",
    "        self.num_class = num_class\n",
    "        self.is_training = is_training\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimizer\n",
    "        self.accuracy\n",
    "        self.correct_num\n",
    "        print('Initializing Image Classification Model!') \n",
    "    \n",
    "    @define_scope\n",
    "    def sigmoid(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        logits = self.build_network\n",
    "        return tf.nn.sigmoid(logits)\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        sigmoid = self.sigmoid\n",
    "        predictions = sigmoid > 0.5\n",
    "        return tf.cast(predictions, tf.int32)\n",
    "    \n",
    "    @define_scope\n",
    "    def cost(self):\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        labels = tf.cast(self.label_holder, tf.float32)\n",
    "        #Cross entropy\n",
    "        #cross_entropy = -tf.reduce_mean(labels*\n",
    "                                       #tf.log(self.prediction), name='cross_entropy')\n",
    "        clipped_value = tf.clip_by_value(self.sigmoid, 1e-10, 0.999)\n",
    "        cross_entropy = -tf.reduce_mean(labels*tf.log(clipped_value) + (1-labels)*tf.log(1-clipped_value))\n",
    "        \n",
    "        #Regularization\n",
    "        l2_loss = cross_entropy\n",
    "        for i in range(len(self.weights)):\n",
    "                l2_loss += tf.nn.l2_loss(self.weights[i])\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    @define_scope\n",
    "    def optimizer(self):\n",
    "        '''Define cross entropy loss function and regularization'''      \n",
    "        l2_loss = self.cost\n",
    "        #Decaying Learning Rate\n",
    "        cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.0001\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 2000, 0.96, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(l2_loss, global_step=cur_step)\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        '''Calculate accuracy for each epoch of training'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        correct_ones = tf.reduce_sum(tf.cast(correct_prediction, tf.int32))\n",
    "        return correct_ones\n",
    "    \n",
    "    def _conv_layer(self, input_tensor, shape, wd, scope, is_pooling=True): \n",
    "        '''\n",
    "        Create a layer of convolutional neural network\n",
    "        Args:\n",
    "        input_tensor:input, [batch_size, height, width, channel]\n",
    "        shape: shape of weights, [filter_width, filter_height, filter_channel, output_channel]\n",
    "        wd: decaying weight\n",
    "        scope: variable scope\n",
    "        '''\n",
    "        #Get the output dim\n",
    "        output_dim = shape[-1]\n",
    "        #Create variables\n",
    "        kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=shape,\n",
    "                                                 wd=wd)\n",
    "        #Compute convolution\n",
    "        conv = tf.nn.conv2d(input_tensor, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = self._variable_on_cpu('biases', [output_dim], tf.constant_initializer(0.0))\n",
    "        z = tf.nn.bias_add(conv, biases)\n",
    "        #Batch normalization\n",
    "        bn = self._batch_normalization(z)\n",
    "        activation = tf.nn.relu(bn, name=scope.name)\n",
    "        \n",
    "        self._activation_summary(activation)\n",
    "        if is_pooling:\n",
    "            pool = tf.nn.max_pool(activation, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "        else:\n",
    "            pool = activation\n",
    "        return pool\n",
    "    \n",
    "    \n",
    "    def _fully_layer(self, input_data, out_dim, scope, wd=0.004):\n",
    "        '''\n",
    "        Create a fully connected layer with specific parameters\n",
    "        Args:\n",
    "        input_data:input tensor, batch_size*width*height*channels\n",
    "        out_dim: output dimension\n",
    "        wd: punishment for l2 regularization\n",
    "        '''\n",
    "        reshape_tensor = tf.reshape(input_data, [self.batch_size, -1], name='reshape_tensor')\n",
    "        in_dim = reshape_tensor.get_shape()[1].value\n",
    "        weights = self._variable_with_weight_decay('weights', shape=[in_dim, out_dim], wd=0.004)\n",
    "        biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
    "        local = tf.nn.relu(tf.matmul(reshape_tensor, weights) + biases, name=scope.name)\n",
    "        self._activation_summary(local)\n",
    "        return local\n",
    "    \n",
    "    def _batch_normalization(self, z):\n",
    "        '''\n",
    "        Calculate batch nomralization of the input data\n",
    "        Args:\n",
    "        z: weighted linear sums of previous neurons\n",
    "        '''\n",
    "        decay = 0.997\n",
    "        epsilon = 0.001\n",
    "        #Scale mean value\n",
    "        bias_shape = z.get_shape()[-1]\n",
    "        gamma = tf.get_variable(name='gamma', initializer=tf.ones(bias_shape))\n",
    "        beta = tf.get_variable(name='beta', initializer=tf.zeros(bias_shape))\n",
    "        moving_mean = tf.get_variable(name='moving_mean', initializer=tf.zeros(bias_shape), trainable=False)\n",
    "        moving_var = tf.get_variable(name='moving_var', initializer=tf.ones(bias_shape), trainable=False)\n",
    "        axis = list(range(len(z.get_shape()) - 1))\n",
    "        #If training\n",
    "        if self.is_training:\n",
    "            batch_mean, batch_var = tf.nn.moments(z, axis)\n",
    "            train_mean = tf.assign(moving_mean,\n",
    "                                   moving_mean * decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.assign(moving_var,\n",
    "                                  moving_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                bn = tf.nn.batch_normalization(z,\n",
    "                                           batch_mean, batch_var, beta, gamma, epsilon)    \n",
    "        else:\n",
    "            bn = tf.nn.batch_normalization(z, moving_mean, moving_var, beta, gamma, epsilon)\n",
    "        return bn\n",
    "    \n",
    "    def _conv_base(self, input_data):\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            shape = [3, 3, 3, 128]\n",
    "            wd = 0\n",
    "            #32*32*32\n",
    "            pool1 = self._conv_layer(input_data, shape, wd, scope)\n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 128, 128]\n",
    "            #16*16*64\n",
    "            pool2 = self._conv_layer(pool1, shape, wd, scope)\n",
    "        # conv3\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 64, 128]\n",
    "            #8*8*128\n",
    "            #pool3 = self._conv_layer(pool2, shape, wd, scope)\n",
    "        activation = pool2\n",
    "        return activation\n",
    "    \n",
    "    def _inception_base(self, input_data):\n",
    "        with tf.variable_scope('Mixed_5c'):\n",
    "            with tf.variable_scope('Branch_0') as scope:\n",
    "                #branch_0 = slim.conv2d(input_data, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                shape = [1, 1, 128, 64]\n",
    "                wd = 0\n",
    "                #8*8*64\n",
    "                branch_0 = self._conv_layer(input_data, shape, wd, scope, is_pooling=False)\n",
    "            with tf.variable_scope('Branch_1') as scope:\n",
    "                with tf.variable_scope('conv_1X1'):\n",
    "                    shape = [1, 1, 128, 48]\n",
    "                    #8*8*48\n",
    "                    branch_1 = self._conv_layer(input_data, shape, wd, scope, is_pooling=False)\n",
    "                with tf.variable_scope('conv_5X5'):\n",
    "                    shape = [5, 5, 48, 64]\n",
    "                    #8*8*64\n",
    "                    branch_1 = self._conv_layer(branch_1, shape, wd, scope, is_pooling=False)\n",
    "            with tf.variable_scope('Branch_2') as scope:\n",
    "                with tf.variable_scope('conv_1X1'):\n",
    "                    shape = [1, 1, 128, 64]\n",
    "                    #8*8*64\n",
    "                    branch_2 = self._conv_layer(input_data, shape, wd, scope, is_pooling=False)\n",
    "                with tf.variable_scope('conv_3X3_0'):\n",
    "                    shape = [3, 3, 64, 96]\n",
    "                    #8*8*96\n",
    "                    branch_2 = self._conv_layer(branch_2, shape, wd, scope, is_pooling=False)\n",
    "                with tf.variable_scope('conv_3X3_1'):\n",
    "                    shape= [3, 3, 96, 96]\n",
    "                    #8*8*96\n",
    "                    branch_2 = self._conv_layer(branch_2, shape, wd, scope, is_pooling=False)\n",
    "            with tf.variable_scope('Branch_3') as scope:\n",
    "                #branch_3 = slim.avg_pool2d(input_data, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                #branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                #8*8*128\n",
    "                branch_3 = tf.nn.avg_pool(input_data, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1],\n",
    "                                                                   padding='SAME', name='pool')\n",
    "                shape = [1, 1, 128, 64]\n",
    "                #8*8*64\n",
    "                branch_3 = self._conv_layer(branch_3, shape, wd, scope, is_pooling=False)\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) \n",
    "        return net\n",
    "    \n",
    "    @define_scope\n",
    "    def build_network(self):\n",
    "        \"\"\"Build the clasification network.\n",
    "        Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "        Returns:\n",
    "        Logits.\n",
    "        \"\"\"\n",
    "        # We instantiate all variables using tf.get_variable() instead of\n",
    "        # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "        # If we only ran this model on a single GPU, we could simplify this function\n",
    "        # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "        #\n",
    "        output = self._conv_base(self.image_holder)\n",
    "        net = self._inception_base(output)\n",
    "        #Flatten the output\n",
    "        with tf.variable_scope('AveragePooling'):\n",
    "            net = tf.nn.avg_pool(net, ksize=[1, 16, 16, 1], strides=[1, 1, 1, 1],\n",
    "                                 padding='VALID', name='pool')\n",
    "        #1X1X288\n",
    "        net = tf.nn.dropout(net, self.keep_prob)\n",
    "        \n",
    "\n",
    "        # local3\n",
    "        #with tf.variable_scope('fully_connected1') as scope:\n",
    "            #local3 = self._fully_layer(net, 384, scope)\n",
    "        #local3_dropout = tf.nn.dropout(local3, self.keep_prob)\n",
    "        # local4\n",
    "        #with tf.variable_scope('fully_connected2') as scope:\n",
    "            #local4 = self._fully_layer(pool3_dropout, 192, scope)\n",
    "            \n",
    "        #local4_dropout = tf.nn.dropout(local4, self.keep_prob)\n",
    "        \n",
    "        # linear layer(WX + b),\n",
    "        # We don't apply softmax here because\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "        #output = local3_dropout\n",
    "        with tf.variable_scope('sigmoid_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [1, 1, 288, self.num_class],\n",
    "                                                  wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_class],\n",
    "                                      tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.conv2d(net, weights, [1, 1, 1, 1], padding='SAME')\n",
    "            sigmoid_linear =  tf.nn.bias_add(conv, biases, name=scope.name)\n",
    "            #remove dimension which is 1\n",
    "            sigmoid_linear = tf.squeeze(sigmoid_linear,[1, 2], name='SpatialSqueeze')\n",
    "            self._activation_summary(sigmoid_linear)\n",
    "            #print(sigmoid_linear)\n",
    "        return sigmoid_linear\n",
    "    \n",
    "    #def _conv_layer(self, )\n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        \"\"\"Helper to create summaries for activations.\n",
    "        Creates a summary that provides a histogram of activations.\n",
    "        Creates a summary that measures the sparsity of activations.\n",
    "        Args:\n",
    "        x: Tensor\n",
    "        Returns:\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "        # session. This helps the clarity of presentation on tensorboard.\n",
    "        tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                          tf.nn.zero_fraction(x))\n",
    "    \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope('weights'):\n",
    "                dtype = tf.float32\n",
    "                var = tf.get_variable(initializer=initializer(shape), dtype=dtype, name=name)\n",
    "            #var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _variable_with_weight_decay(self, name, shape, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        dtype = tf.float32\n",
    "        var = self._variable_on_cpu(name, shape,\n",
    "                               tf.contrib.layers.xavier_initializer())\n",
    "        if wd is not None and wd > 0:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            #tf.add_to_collection('losses', weight_decay)\n",
    "            self.weights.append(weight_decay)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义输入张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Image Classification Model!\n",
      "Initializing Image Classification Model!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    model = imageClassification(image_holder, label_holder, keep_prob, batch_size, num_class =1)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    test_model = imageClassification(image_holder, label_holder, keep_prob, batch_size, 1, False)\n",
    "    #with tf.variable_scope('VGG'):\n",
    "        #model_train = imageClassification(image_holder, label_holder, keep_prob, batch_size=64, num_class =1)\n",
    "    #with tf.variable_scope('VGG', reuse=True):\n",
    "        #model_test = imageClassification(image_holder, label_holder, keep_prob, batch_size=16, num_class =1) \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "path = 'C:/Users/onlooker/Documents/deeplearning_projects/cats_dogs_classification/dataset'\n",
    "training_set = train_datagen.flow_from_directory(path+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(path+'/test_set', shuffle=False,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.431856\n",
      "Cost: 0.366955\n",
      "Cost: 0.400778\n",
      "Cost: 0.383108\n",
      "Cost: 0.328595\n",
      "Cost: 0.382513\n",
      "Cost: 0.383736\n",
      "Cost: 0.387899\n",
      "Cost: 0.371837\n",
      "Cost: 0.3643\n",
      "Cost: 0.324861\n",
      "Cost: 0.246209\n",
      "Cost: 0.257335\n",
      "Cost: 0.341806\n",
      "Cost: 0.265123\n",
      "Cost: 0.46841\n",
      "Cost: 0.314234\n",
      "Cost: 0.32488\n",
      "Cost: 0.46152\n",
      "Cost: 0.38794\n",
      "Cost: 0.323661\n",
      "Cost: 0.279217\n",
      "Cost: 0.307353\n",
      "Cost: 0.378528\n",
      "Cost: 0.454374\n",
      "Cost: 0.336229\n",
      "Cost: 0.425698\n",
      "Cost: 0.32699\n",
      "Cost: 0.383612\n",
      "Cost: 0.38041\n",
      "Cost: 0.370629\n",
      "Cost: 0.365638\n",
      "Cost: 0.352378\n",
      "Cost: 0.252727\n",
      "Cost: 0.483972\n",
      "Cost: 0.402577\n",
      "Cost: 0.346532\n",
      "Cost: 0.329065\n",
      "Cost: 0.334837\n",
      "Cost: 0.334728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-e6df02ab05d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[1;31m#数据传递给tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mimage_holder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_holder\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[1;31m#每50次计算准确度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_steps = 2001\n",
    "epochs = 10\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    #If there exists models in advance, load them\n",
    "    if os.path.exists(\"./Model/checkpoint\"): # 注意此处路径前添加\"./\" ')\n",
    "        saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 0.5}\n",
    "            _, loss = sess.run([model.optimizer, model.cost], feed_dict=feed_dict)\n",
    "            if step%200 == 0:\n",
    "                #每50次计算准确度\n",
    "                #result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', loss) \n",
    "                #保存模型\n",
    "        saver.save(sess, \"Model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Result.....\n",
      "Accuracy: 0.8495\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    print('Testing Result.....')  \n",
    "    count = 0\n",
    "    for i in range(40):\n",
    "        batch_data, batch_labels = test_set.next()\n",
    "        #print(batch_data.shape)\n",
    "        #print(i)\n",
    "        batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "        feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 1}\n",
    "        c, a = sess.run([test_model.correct_num, test_model.prediction], feed_dict=feed_dict)\n",
    "        #print(a)\n",
    "        count += c\n",
    "    result = count*1.0/2000\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
