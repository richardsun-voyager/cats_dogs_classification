{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "path = 'C:/Users/onlooker/Documents/deeplearning_projects/cats_dogs_classification/dataset'\n",
    "training_set = train_datagen.flow_from_directory(path+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(path+'/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 2000,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建简单卷积网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建权重变量\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.02)\n",
    "  return tf.Variable(initial)\n",
    "#创建偏差\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.001, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#卷积函数\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#池化函数\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #变量定义\n",
    "    #定义输入数据\n",
    "    x = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "    #期望输出标签\n",
    "    y_ = tf.placeholder(tf.float32, [None,1])\n",
    "    #丢弃因子\n",
    "    keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#两层的卷积网络\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        #第一层卷积\n",
    "        W_conv1 = weight_variable([3, 3, 3, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        #卷积后采用Relu函数激活\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    #进行池化\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        #第二层卷积\n",
    "        W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "        b_conv2 = bias_variable([32])\n",
    "        #卷积后采用Relu函数激活\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        #进行池化\n",
    "        h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全连接层\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        W_fc1 = weight_variable([16*16 * 32, 128])\n",
    "        b_fc1 = bias_variable([128])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 16*16*32])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    #丢弃，为了增强泛化能力\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#输出层，softmax\n",
    "#全连接层\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W_fc2 = weight_variable([128, 1])\n",
    "        b_fc2 = bias_variable([1])\n",
    "    y_conv=tf.nn.sigmoid(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数以及梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    #损失函数\n",
    "    cross_entropy = -tf.reduce_mean(y_*tf.log(y_conv) + (1-y_)*tf.log(1-y_conv))\n",
    "    #梯度下降法，使用Adam算法\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "     #Decaying Learning Rate\n",
    "    #cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    #starter_learning_rate = 0.001\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 100000, 0.96, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy, global_step=cur_step)\n",
    "    #计算准确度\n",
    "    preds = y_conv > 0.5\n",
    "    correct_prediction = tf.equal(tf.cast(preds, tf.int32), tf.cast(y_, tf.int32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.693033\n",
      "Cost: 0.693802\n",
      "Cost: 0.688323\n",
      "Cost: 0.683102\n",
      "Cost: 0.678234\n",
      "Cost: 0.695801\n",
      "Cost: 0.610721\n",
      "Cost: 0.661946\n",
      "Cost: 0.604205\n",
      "Cost: 0.617408\n",
      "Cost: 0.65438\n",
      "Cost: 0.551946\n",
      "Cost: 0.621788\n",
      "Cost: 0.572949\n",
      "Cost: 0.732513\n",
      "Cost: 0.618643\n",
      "Cost: 0.692049\n",
      "Cost: 0.610078\n",
      "Cost: 0.606186\n",
      "Cost: 0.59901\n",
      "Cost: 0.495944\n",
      "Cost: 0.676135\n",
      "Cost: 0.678831\n",
      "Cost: 0.574497\n",
      "Cost: 0.58973\n",
      "Cost: 0.419729\n",
      "Cost: 0.563035\n",
      "Cost: 0.665094\n",
      "Cost: 0.571348\n",
      "Cost: 0.547471\n",
      "Cost: 0.521333\n",
      "Cost: 0.487861\n",
      "Cost: 0.51393\n",
      "Cost: 0.619993\n",
      "Cost: 0.601518\n",
      "Cost: 0.619723\n",
      "Cost: 0.563748\n",
      "Cost: 0.481831\n",
      "Cost: 0.406371\n",
      "Cost: 0.464723\n",
      "Cost: 0.528513\n",
      "Cost: 0.478059\n",
      "Cost: 0.8004\n",
      "Cost: 0.550918\n",
      "Cost: 0.614105\n",
      "Cost: 0.499858\n",
      "Cost: 0.6156\n",
      "Cost: 0.49121\n",
      "Cost: 0.491891\n",
      "Cost: 0.648234\n",
      "Cost: 0.578275\n",
      "Cost: 0.526935\n",
      "Cost: 0.581431\n",
      "Cost: 0.500393\n",
      "Cost: 0.466093\n",
      "Cost: 0.496809\n",
      "Cost: 0.526293\n",
      "Cost: 0.578282\n",
      "Cost: 0.49801\n",
      "Cost: 0.505988\n",
      "Cost: 0.545323\n",
      "Cost: 0.507415\n",
      "Cost: 0.613997\n",
      "Cost: 0.535357\n",
      "Cost: 0.661268\n",
      "Cost: 0.627345\n",
      "Cost: 0.386283\n",
      "Cost: 0.535929\n",
      "Cost: 0.521449\n",
      "Cost: 0.444926\n",
      "Cost: 0.614969\n",
      "Cost: 0.395182\n",
      "Cost: 0.519724\n",
      "Cost: 0.525701\n",
      "Cost: 0.398549\n",
      "Cost: 0.558886\n",
      "Cost: 0.557766\n",
      "Cost: 0.446565\n",
      "Cost: 0.359318\n",
      "Cost: 0.508404\n",
      "Cost: 0.521427\n",
      "Cost: 0.495395\n",
      "Cost: 0.421318\n",
      "Cost: 0.578754\n",
      "Cost: 0.54922\n",
      "Cost: 0.636934\n",
      "Cost: 0.392021\n",
      "Cost: 0.467787\n",
      "Cost: 0.622139\n",
      "Cost: 0.466876\n",
      "Cost: 0.483991\n",
      "Cost: 0.476072\n",
      "Cost: 0.4099\n",
      "Cost: 0.389592\n",
      "Cost: 0.481909\n",
      "Cost: 0.508234\n",
      "Cost: 0.373792\n",
      "Cost: 0.380066\n",
      "Cost: 0.428105\n",
      "Cost: 0.47553\n",
      "Cost: 0.556949\n",
      "Cost: 0.438244\n",
      "Cost: 0.539617\n",
      "Cost: 0.421227\n",
      "Cost: 0.428564\n",
      "Cost: 0.432515\n",
      "Cost: 0.634613\n",
      "Cost: 0.377743\n",
      "Cost: 0.507221\n",
      "Cost: 0.413547\n",
      "Cost: 0.496805\n",
      "Cost: 0.514184\n",
      "Cost: 0.60821\n",
      "Cost: 0.377706\n",
      "Cost: 0.431842\n",
      "Cost: 0.444103\n",
      "Cost: 0.446685\n",
      "Cost: 0.263014\n",
      "Cost: 0.387169\n",
      "Cost: 0.454899\n",
      "Cost: 0.451231\n",
      "Cost: 0.461526\n",
      "Cost: 0.506175\n",
      "Cost: 0.509389\n",
      "Cost: 0.465236\n",
      "Cost: 0.377287\n",
      "Cost: 0.436451\n",
      "Cost: 0.370444\n",
      "Cost: 0.491723\n",
      "Cost: 0.422472\n",
      "Cost: 0.467691\n",
      "Cost: 0.395852\n",
      "Cost: 0.449166\n",
      "Cost: 0.31697\n",
      "Cost: 0.433714\n",
      "Cost: 0.345757\n",
      "Cost: 0.29379\n",
      "Cost: 0.411937\n",
      "Cost: 0.496075\n",
      "Cost: 0.370115\n",
      "Cost: 0.322751\n",
      "Cost: 0.406658\n",
      "Cost: 0.538773\n",
      "Cost: 0.448874\n",
      "Cost: 0.525085\n",
      "Cost: 0.4879\n",
      "Cost: 0.483946\n",
      "Cost: 0.634587\n",
      "Cost: 0.442923\n",
      "Cost: 0.486578\n",
      "Cost: 0.423299\n",
      "Cost: 0.430854\n",
      "Cost: 0.34554\n",
      "Cost: 0.316409\n",
      "Cost: 0.471406\n",
      "Cost: 0.236148\n",
      "Cost: 0.350627\n",
      "Cost: 0.462728\n",
      "Cost: 0.621352\n",
      "Cost: 0.351705\n",
      "Cost: 0.414419\n",
      "Cost: 0.45032\n",
      "Cost: 0.586433\n",
      "Cost: 0.545334\n",
      "Cost: 0.48401\n",
      "Cost: 0.559265\n",
      "Cost: 0.559203\n",
      "Cost: 0.350017\n",
      "Cost: 0.373479\n",
      "Cost: 0.348726\n",
      "Cost: 0.53484\n",
      "Cost: 0.503076\n",
      "Cost: 0.356028\n",
      "Cost: 0.462646\n",
      "Cost: 0.397847\n",
      "Cost: 0.527781\n",
      "Cost: 0.430848\n",
      "Cost: 0.293478\n",
      "Cost: 0.363693\n",
      "Cost: 0.335543\n",
      "Cost: 0.465658\n",
      "Cost: 0.411576\n",
      "Cost: 0.387449\n",
      "Cost: 0.333781\n",
      "Cost: 0.335408\n",
      "Cost: 0.386791\n",
      "Cost: 0.333371\n",
      "Cost: 0.410835\n",
      "Cost: 0.306393\n",
      "Cost: 0.476032\n",
      "Cost: 0.413138\n",
      "Cost: 0.53736\n",
      "Cost: 0.469758\n",
      "Cost: 0.408021\n",
      "Cost: 0.415458\n",
      "Cost: 0.454774\n",
      "Cost: 0.343122\n",
      "Cost: 0.47296\n",
      "Cost: 0.370019\n",
      "Cost: 0.480485\n",
      "Cost: 0.490725\n",
      "Cost: 0.505137\n",
      "Cost: 0.526947\n",
      "Cost: 0.479187\n",
      "Cost: 0.387331\n",
      "Cost: 0.395233\n",
      "Cost: 0.379444\n",
      "Cost: 0.382327\n",
      "Cost: 0.495008\n",
      "Cost: 0.492271\n",
      "Cost: 0.441659\n",
      "Cost: 0.456357\n",
      "Cost: 0.421911\n",
      "Cost: 0.485543\n",
      "Cost: 0.371829\n",
      "Cost: 0.239369\n",
      "Cost: 0.511817\n",
      "Cost: 0.506509\n",
      "Cost: 0.358318\n",
      "Cost: 0.365972\n",
      "Cost: 0.458851\n",
      "Cost: 0.310547\n",
      "Cost: 0.568085\n",
      "Cost: 0.425106\n",
      "Cost: 0.377955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b97e27eba52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[1;31m#产生训练用样本集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[1;31m#数据传递给tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             img = load_img(os.path.join(self.directory, fname),\n\u001b[1;32m   1032\u001b[0m                            \u001b[0mgrayscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                            target_size=self.target_size)\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mhw_tuple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mhw_tuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhw_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown resampling filter\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                         \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "epochs = 50\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:0.5}\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            if step%100 == 0:\n",
    "                #每50次计算准确度\n",
    "                result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', result)   \n",
    "    print('Testing Result.....')       \n",
    "    batch_data, batch_labels = test_set.next()\n",
    "    batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "    feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:1}\n",
    "    result = sess.run(accuracy, feed_dict)\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "TOWER_NAME = 'tower'\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "           # with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define cifar model\n",
    "class imageClassification:\n",
    "    '''Define a basic model for image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, image_holder, label_holder, batch_size=32, num_class=10):\n",
    "        self.image_holder = image_holder\n",
    "        self.label_holder = label_holder\n",
    "        self.num_class = num_class\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.accuracy\n",
    "        self.correct_num\n",
    "        print('Initializing Image Classification Model!') \n",
    "    \n",
    "    @define_scope\n",
    "    def sigmoid(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        logits = self.inference\n",
    "        return tf.nn.sigmoid(logits)\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        sigmoid = self.sigmoid\n",
    "        predictions = sigmoid > 0.5\n",
    "        return tf.cast(predictions, tf.int32)\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        '''Define cross entropy loss function and regularization'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        labels = tf.cast(self.label_holder, tf.float32)\n",
    "        #Cross entropy\n",
    "        #cross_entropy = -tf.reduce_mean(labels*\n",
    "                                       #tf.log(self.prediction), name='cross_entropy')\n",
    "        cross_entropy = -tf.reduce_mean(labels*tf.log(self.sigmoid) + (1-labels)*tf.log(1-self.sigmoid))\n",
    "        \n",
    "        #Regularization\n",
    "        l2_loss = cross_entropy\n",
    "        for i in range(len(self.weights)):\n",
    "                l2_loss += tf.nn.l2_loss(self.weights[i])\n",
    "\n",
    "        #Decaying Learning Rate\n",
    "        cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.4\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 100000, 0.96, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(l2_loss, global_step=cur_step)\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        '''Calculate accuracy for each epoch of training'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        correct_ones = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "        return correct_ones\n",
    "    \n",
    "    @define_scope\n",
    "    def inference(self):\n",
    "        \"\"\"Build the clasification model.\n",
    "        Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "        Returns:\n",
    "        Logits.\n",
    "        \"\"\"\n",
    "        # We instantiate all variables using tf.get_variable() instead of\n",
    "        # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "        # If we only ran this model on a single GPU, we could simplify this function\n",
    "        # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "        #\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=[3, 3, 3, 64],\n",
    "                                                 wd=0.0)\n",
    "            conv = tf.nn.conv2d(self.image_holder, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv1)\n",
    "        # pool1\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "        \n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=[3, 3, 64, 64],                                                 \n",
    "                                                 wd=0.0)\n",
    "            conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv2)      \n",
    "\n",
    "        # pool2\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "        \n",
    "        # local3\n",
    "        with tf.variable_scope('fully_connected1') as scope:\n",
    "            # Move everything into depth so we can perform a single matrix multiply.\n",
    "            reshape_tensor = tf.reshape(pool2, [self.batch_size, -1], name='reshape_tensor')\n",
    "            dim = reshape_tensor.get_shape()[1].value\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                                  wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape_tensor, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local3)\n",
    "        # local4\n",
    "        with tf.variable_scope('fully_connected2') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                                  wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "            local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local4)\n",
    "        \n",
    "        # linear layer(WX + b),\n",
    "        # We don't apply softmax here because\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "        with tf.variable_scope('softmax_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [192, self.num_class],\n",
    "                                                  wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_class],\n",
    "                                      tf.constant_initializer(0.0))\n",
    "            softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(softmax_linear)\n",
    "        \n",
    "        return softmax_linear\n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        \"\"\"Helper to create summaries for activations.\n",
    "        Creates a summary that provides a histogram of activations.\n",
    "        Creates a summary that measures the sparsity of activations.\n",
    "        Args:\n",
    "        x: Tensor\n",
    "        Returns:\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "        # session. This helps the clarity of presentation on tensorboard.\n",
    "        tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                          tf.nn.zero_fraction(x))\n",
    "        \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope('weights'):\n",
    "                dtype = tf.float32\n",
    "                var = tf.get_variable(initializer=initializer(shape), dtype=dtype, name=name)\n",
    "            #var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _variable_with_weight_decay(self, name, shape, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        dtype = tf.float32\n",
    "        var = self._variable_on_cpu(name,\n",
    "                               shape,\n",
    "                               tf.contrib.layers.xavier_initializer())\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            #tf.add_to_collection('losses', weight_decay)\n",
    "            self.weights.append(weight_decay)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Image Classification Model!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    batch_size = 128\n",
    "    num_steps = 8001\n",
    "    image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [batch_size])\n",
    "    model = imageClassification(image_holder, label_holder, batch_size, num_class =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
