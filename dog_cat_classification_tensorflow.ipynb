{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构\n",
    "\n",
    "参看VGG网络结构，适当减少了参数尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "TOWER_NAME = 'tower'\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "           # with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define dog cat classification model\n",
    "class imageClassification:\n",
    "    '''Define a basic model for image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, image_holder, label_holder, keep_prob, batch_size=32, num_class=1, is_training=True):\n",
    "        self.image_holder = image_holder\n",
    "        self.label_holder = label_holder\n",
    "        self.num_class = num_class\n",
    "        self.is_training = is_training\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimizer\n",
    "        self.accuracy\n",
    "        self.correct_num\n",
    "        print('Initializing Image Classification Model!') \n",
    "    \n",
    "    @define_scope\n",
    "    def sigmoid(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        logits = self.build_network\n",
    "        return tf.nn.sigmoid(logits)\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        sigmoid = self.sigmoid\n",
    "        predictions = sigmoid > 0.5\n",
    "        return tf.cast(predictions, tf.int32)\n",
    "    \n",
    "    @define_scope\n",
    "    def cost(self):\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        labels = tf.cast(self.label_holder, tf.float32)\n",
    "        #Cross entropy\n",
    "        #cross_entropy = -tf.reduce_mean(labels*\n",
    "                                       #tf.log(self.prediction), name='cross_entropy')\n",
    "        clipped_value = tf.clip_by_value(self.sigmoid, 1e-10, 0.999)\n",
    "        cross_entropy = -tf.reduce_mean(labels*tf.log(clipped_value) + (1-labels)*tf.log(1-clipped_value))\n",
    "        \n",
    "        #Regularization\n",
    "        l2_loss = cross_entropy\n",
    "        for i in range(len(self.weights)):\n",
    "                l2_loss += tf.nn.l2_loss(self.weights[i])\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    @define_scope\n",
    "    def optimizer(self):\n",
    "        '''Define cross entropy loss function and regularization'''      \n",
    "        l2_loss = self.cost\n",
    "        #Decaying Learning Rate\n",
    "        cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.008\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 3000, 0.96, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(l2_loss, global_step=cur_step)\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        '''Calculate accuracy for each epoch of training'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        correct_ones = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "        return correct_ones\n",
    "    \n",
    "    def _conv_layer(self, input_tensor, shape, wd, scope): \n",
    "        '''\n",
    "        Create a layer of convolutional neural network\n",
    "        Args:\n",
    "        input_tensor:input, [batch_size, height, width, channel]\n",
    "        shape: shape of weights, [filter_width, filter_height, filter_channel, output_channel]\n",
    "        wd: decaying weight\n",
    "        scope: variable scope\n",
    "        '''\n",
    "        #Get the output dim\n",
    "        output_dim = shape[-1]\n",
    "        #Create variables\n",
    "        kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=shape,\n",
    "                                                 wd=wd)\n",
    "        #Compute convolution\n",
    "        conv = tf.nn.conv2d(input_tensor, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = self._variable_on_cpu('biases', [output_dim], tf.constant_initializer(0.0))\n",
    "        z = tf.nn.bias_add(conv, biases)\n",
    "        #Batch normalization\n",
    "        bn = self._batch_normalization(z)\n",
    "        activation = tf.nn.relu(bn, name=scope.name)\n",
    "        \n",
    "        self._activation_summary(activation)\n",
    "        pool = tf.nn.max_pool(activation, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "        return pool\n",
    "    \n",
    "    def _fully_layer(self, input_data, out_dim, scope, wd=0.004):\n",
    "        '''\n",
    "        Create a fully connected layer with specific parameters\n",
    "        Args:\n",
    "        input_data:input tensor, batch_size*width*height*channels\n",
    "        out_dim: output dimension\n",
    "        wd: punishment for l2 regularization\n",
    "        '''\n",
    "        reshape_tensor = tf.reshape(input_data, [self.batch_size, -1], name='reshape_tensor')\n",
    "        in_dim = reshape_tensor.get_shape()[1].value\n",
    "        weights = self._variable_with_weight_decay('weights', shape=[in_dim, out_dim], wd=0.004)\n",
    "        biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
    "        local = tf.nn.relu(tf.matmul(reshape_tensor, weights) + biases, name=scope.name)\n",
    "        self._activation_summary(local)\n",
    "        return local\n",
    "    \n",
    "    def _batch_normalization(self, z):\n",
    "        '''\n",
    "        Calculate batch nomralization of the input data\n",
    "        Args:\n",
    "        z: weighted linear sums of previous neurons\n",
    "        '''\n",
    "        decay = 0.9\n",
    "        epsilon = 0.001\n",
    "        #Scale mean value\n",
    "        bias_shape = z.get_shape()[-1]\n",
    "        gamma = tf.get_variable(name='gamma', initializer=tf.ones(bias_shape))\n",
    "        beta = tf.get_variable(name='beta', initializer=tf.zeros(bias_shape))\n",
    "        moving_mean = tf.get_variable(name='moving_mean', initializer=tf.zeros(bias_shape), trainable=False)\n",
    "        moving_var = tf.get_variable(name='moving_var', initializer=tf.ones(bias_shape), trainable=False)\n",
    "        axis = list(range(len(z.get_shape()) - 1))\n",
    "        #If training\n",
    "        if self.is_training:\n",
    "            batch_mean, batch_var = tf.nn.moments(z, axis)\n",
    "            train_mean = tf.assign(moving_mean,\n",
    "                                   moving_mean * decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.assign(moving_var,\n",
    "                                  moving_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                bn = tf.nn.batch_normalization(z,\n",
    "                                           batch_mean, batch_var, beta, gamma, epsilon)    \n",
    "        else:\n",
    "            bn = tf.nn.batch_normalization(z, moving_mean, moving_var, beta, gamma, epsilon)\n",
    "        return bn\n",
    "    \n",
    "    @define_scope\n",
    "    def build_network(self):\n",
    "        \"\"\"Build the clasification network.\n",
    "        Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "        Returns:\n",
    "        Logits.\n",
    "        \"\"\"\n",
    "        # We instantiate all variables using tf.get_variable() instead of\n",
    "        # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "        # If we only ran this model on a single GPU, we could simplify this function\n",
    "        # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "        #\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            shape = [3, 3, 3, 64]\n",
    "            wd = 0\n",
    "            pool1 = self._conv_layer(self.image_holder, shape, wd, scope)\n",
    "        \n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 64, 128]\n",
    "            pool2 = self._conv_layer(pool1, shape, wd, scope)\n",
    "        \n",
    "        # conv3\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            wd = 0\n",
    "            shape = [3, 3, 128, 128]\n",
    "            pool3 = self._conv_layer(pool2, shape, wd, scope)\n",
    "        \n",
    "        pool3_dropout = tf.nn.dropout(pool3, self.keep_prob)\n",
    "\n",
    "        # local3\n",
    "        with tf.variable_scope('fully_connected1') as scope:\n",
    "            local3 = self._fully_layer(pool3_dropout, 384, scope)\n",
    "        local3_dropout = tf.nn.dropout(local3, self.keep_prob)\n",
    "        # local4\n",
    "        #with tf.variable_scope('fully_connected2') as scope:\n",
    "            #local4 = self._fully_layer(pool3_dropout, 192, scope)\n",
    "            \n",
    "        #local4_dropout = tf.nn.dropout(local4, self.keep_prob)\n",
    "        \n",
    "        # linear layer(WX + b),\n",
    "        # We don't apply softmax here because\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "        #output = local3_dropout\n",
    "        with tf.variable_scope('softmax_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [384, self.num_class],\n",
    "                                                  wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_class],\n",
    "                                      tf.constant_initializer(0.0))\n",
    "            softmax_linear = tf.add(tf.matmul(local3_dropout, weights), biases, name=scope.name)\n",
    "            self._activation_summary(softmax_linear)\n",
    "        return softmax_linear\n",
    "    \n",
    "    #def _conv_layer(self, )\n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        \"\"\"Helper to create summaries for activations.\n",
    "        Creates a summary that provides a histogram of activations.\n",
    "        Creates a summary that measures the sparsity of activations.\n",
    "        Args:\n",
    "        x: Tensor\n",
    "        Returns:\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "        # session. This helps the clarity of presentation on tensorboard.\n",
    "        tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                          tf.nn.zero_fraction(x))\n",
    "    \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope('weights'):\n",
    "                dtype = tf.float32\n",
    "                var = tf.get_variable(initializer=initializer(shape), dtype=dtype, name=name)\n",
    "            #var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _variable_with_weight_decay(self, name, shape, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        dtype = tf.float32\n",
    "        var = self._variable_on_cpu(name, shape,\n",
    "                               tf.contrib.layers.xavier_initializer())\n",
    "        if wd is not None and wd > 0:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            #tf.add_to_collection('losses', weight_decay)\n",
    "            self.weights.append(weight_decay)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义输入张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Image Classification Model!\n",
      "Initializing Image Classification Model!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "#tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    model = imageClassification(image_holder, label_holder, keep_prob, batch_size, num_class =1)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    test_model = imageClassification(image_holder, label_holder, keep_prob, batch_size, 1, False)\n",
    "    #with tf.variable_scope('VGG'):\n",
    "        #model_train = imageClassification(image_holder, label_holder, keep_prob, batch_size=64, num_class =1)\n",
    "    #with tf.variable_scope('VGG', reuse=True):\n",
    "        #model_test = imageClassification(image_holder, label_holder, keep_prob, batch_size=16, num_class =1) \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "path = 'C:/Users/onlooker/Documents/deeplearning_projects/cats_dogs_classification/dataset'\n",
    "training_set = train_datagen.flow_from_directory(path+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(path+'/test_set', shuffle=False,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n",
      "Cost: 1.43079\n",
      "Cost: 1.39677\n",
      "Cost: 1.38023\n",
      "Cost: 1.28162\n",
      "Cost: 1.32992\n",
      "Cost: 1.41762\n",
      "Cost: 1.22463\n",
      "Cost: 1.12321\n",
      "Cost: 1.10906\n",
      "Cost: 1.02226\n",
      "Cost: 1.11853\n",
      "Cost: 1.11234\n",
      "Cost: 1.03971\n",
      "Cost: 0.961666\n",
      "Cost: 1.0194\n",
      "Cost: 1.04534\n",
      "Cost: 0.88491\n",
      "Cost: 1.0071\n",
      "Cost: 0.969015\n",
      "Cost: 0.976313\n",
      "Cost: 0.908817\n",
      "Cost: 0.84735\n",
      "Cost: 0.90073\n",
      "Cost: 0.691915\n",
      "Cost: 0.870706\n",
      "Cost: 0.915623\n",
      "Cost: 0.6993\n",
      "Cost: 0.898661\n",
      "Cost: 0.744062\n",
      "Cost: 0.663984\n",
      "Cost: 0.807403\n",
      "Cost: 0.724455\n",
      "Cost: 0.837301\n",
      "Cost: 0.754099\n",
      "Cost: 0.782834\n",
      "Cost: 0.764211\n",
      "Cost: 0.669651\n",
      "Cost: 0.750627\n",
      "Cost: 0.579813\n",
      "Cost: 0.629006\n",
      "Cost: 0.708331\n",
      "Cost: 0.606874\n",
      "Cost: 0.603656\n",
      "Cost: 0.659163\n",
      "Cost: 0.66272\n",
      "Cost: 0.595127\n",
      "Cost: 0.630878\n",
      "Cost: 0.592314\n",
      "Cost: 0.61475\n",
      "Cost: 0.565163\n",
      "Cost: 0.593267\n",
      "Cost: 0.489533\n",
      "Cost: 0.693938\n",
      "Cost: 0.508586\n",
      "Cost: 0.608764\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_steps = 2001\n",
    "epochs = 5\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    #If there exists models in advance, load them\n",
    "    if os.path.exists(\"./Model/checkpoint\"): # 注意此处路径前添加\"./\" ')\n",
    "        saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 0.5}\n",
    "            _, loss = sess.run([model.optimizer, model.cost], feed_dict=feed_dict)\n",
    "            if step%200 == 0:\n",
    "                #每50次计算准确度\n",
    "                #result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', loss) \n",
    "                #保存模型\n",
    "        saver.save(sess, \"Model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n",
      "Testing Result.....\n",
      "Accuracy: 0.8735\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    print('Testing Result.....')  \n",
    "    count = 0\n",
    "    for i in range(40):\n",
    "        batch_data, batch_labels = test_set.next()\n",
    "        #print(batch_data.shape)\n",
    "        #print(i)\n",
    "        batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "        feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 1}\n",
    "        c = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += c\n",
    "    result = count*1.0/2000\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
