{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 2002,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建卷积网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建权重变量\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.02)\n",
    "  return tf.Variable(initial)\n",
    "#创建偏差\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.001, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#卷积函数\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#池化函数\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #变量定义\n",
    "    #定义输入数据\n",
    "    x = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "    #期望输出标签\n",
    "    y_ = tf.placeholder(tf.float32, [None,1])\n",
    "    #丢弃因子\n",
    "    keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        #第一层卷积\n",
    "        W_conv1 = weight_variable([3, 3, 3, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        #卷积后采用Relu函数激活\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    #进行池化\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        #第二层卷积\n",
    "        W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "        b_conv2 = bias_variable([32])\n",
    "        #卷积后采用Relu函数激活\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        #进行池化\n",
    "        h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variables.Variable at 0x188197dc198>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2/MaxPool:0' shape=(?, 16, 16, 32) dtype=float32>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全连接层\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        W_fc1 = weight_variable([16*16 * 32, 128])\n",
    "        b_fc1 = bias_variable([128])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 16*16*32])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fully_connected/Relu:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#丢弃，为了增强泛化能力\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#输出层，softmax\n",
    "#全连接层\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W_fc2 = weight_variable([128, 1])\n",
    "        b_fc2 = bias_variable([1])\n",
    "    y_conv=tf.nn.sigmoid(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数以及梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    #损失函数\n",
    "    cross_entropy = -tf.reduce_mean(y_*tf.log(y_conv) + (1-y_)*tf.log(1-y_conv))\n",
    "    #梯度下降法，使用Adam算法\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "     #Decaying Learning Rate\n",
    "    #cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    #starter_learning_rate = 0.001\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 100000, 0.96, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy, global_step=cur_step)\n",
    "    #计算准确度\n",
    "    preds = y_conv > 0.5\n",
    "    correct_prediction = tf.equal(tf.cast(preds, tf.int32), tf.cast(y_, tf.int32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.693007\n",
      "Cost: 0.691502\n",
      "Cost: 0.689338\n",
      "Cost: 0.700539\n",
      "Cost: 0.685761\n",
      "Cost: 0.63397\n",
      "Cost: 0.670364\n",
      "Cost: 0.669262\n",
      "Cost: 0.659475\n",
      "Cost: 0.641019\n",
      "Cost: 0.63135\n",
      "Cost: 0.631915\n",
      "Cost: 0.63762\n",
      "Cost: 0.681392\n",
      "Cost: 0.662226\n",
      "Cost: 0.621458\n",
      "Cost: 0.637389\n",
      "Cost: 0.567379\n",
      "Cost: 0.587076\n",
      "Cost: 0.63213\n",
      "Cost: 0.684681\n",
      "Cost: 0.707753\n",
      "Cost: 0.61392\n",
      "Cost: 0.611486\n",
      "Cost: 0.577305\n",
      "Cost: 0.63776\n",
      "Cost: 0.552176\n",
      "Cost: 0.645295\n",
      "Cost: 0.662904\n",
      "Cost: 0.634555\n",
      "Cost: 0.632221\n",
      "Cost: 0.49478\n",
      "Cost: 0.527905\n",
      "Cost: 0.534481\n",
      "Cost: 0.582598\n",
      "Cost: 0.548433\n",
      "Cost: 0.420304\n",
      "Cost: 0.552564\n",
      "Cost: 0.516261\n",
      "Cost: 0.66945\n",
      "Cost: 0.677477\n",
      "Cost: 0.636271\n",
      "Cost: 0.502658\n",
      "Cost: 0.778908\n",
      "Cost: 0.507724\n",
      "Cost: 0.53839\n",
      "Cost: 0.455044\n",
      "Cost: 0.51519\n",
      "Cost: 0.675639\n",
      "Cost: 0.407274\n",
      "Cost: 0.721305\n",
      "Cost: 0.51435\n",
      "Cost: 0.652092\n",
      "Cost: 0.574896\n",
      "Cost: 0.54661\n",
      "Cost: 0.498424\n",
      "Cost: 0.442943\n",
      "Cost: 0.508728\n",
      "Cost: 0.552254\n",
      "Cost: 0.458525\n",
      "Cost: 0.578653\n",
      "Cost: 0.415252\n",
      "Cost: 0.410941\n",
      "Cost: 0.564578\n",
      "Cost: 0.535981\n",
      "Cost: 0.380072\n",
      "Cost: 0.554747\n",
      "Cost: 0.485559\n",
      "Cost: 0.542438\n",
      "Cost: 0.565608\n",
      "Cost: 0.449975\n",
      "Cost: 0.396461\n",
      "Cost: 0.596594\n",
      "Cost: 0.477782\n",
      "Cost: 0.451513\n",
      "Cost: 0.622621\n",
      "Cost: 0.481079\n",
      "Cost: 0.397692\n",
      "Cost: 0.527735\n",
      "Cost: 0.635126\n",
      "Cost: 0.375099\n",
      "Cost: 0.542512\n",
      "Cost: 0.469653\n",
      "Cost: 0.465417\n",
      "Cost: 0.469696\n",
      "Cost: 0.62437\n",
      "Cost: 0.52211\n",
      "Cost: 0.400721\n",
      "Cost: 0.533677\n",
      "Cost: 0.417356\n",
      "Cost: 0.535983\n",
      "Cost: 0.525202\n",
      "Cost: 0.483141\n",
      "Cost: 0.435235\n",
      "Cost: 0.409825\n",
      "Cost: 0.535602\n",
      "Cost: 0.451716\n",
      "Cost: 0.333031\n",
      "Cost: 0.530689\n",
      "Cost: 0.495884\n",
      "Cost: 0.351733\n",
      "Cost: 0.395527\n",
      "Cost: 0.441213\n",
      "Cost: 0.489854\n",
      "Cost: 0.436013\n",
      "Cost: 0.551961\n",
      "Cost: 0.541828\n",
      "Cost: 0.487716\n",
      "Cost: 0.41054\n",
      "Cost: 0.356376\n",
      "Cost: 0.464076\n",
      "Cost: 0.4823\n",
      "Cost: 0.490717\n",
      "Cost: 0.43645\n",
      "Cost: 0.4242\n",
      "Cost: 0.403252\n",
      "Cost: 0.387951\n",
      "Cost: 0.676322\n",
      "Cost: 0.376347\n",
      "Cost: 0.44545\n",
      "Cost: 0.457787\n",
      "Cost: 0.426864\n",
      "Cost: 0.503539\n",
      "Cost: 0.587553\n",
      "Cost: 0.468143\n",
      "Cost: 0.451768\n",
      "Cost: 0.517069\n",
      "Cost: 0.581011\n",
      "Cost: 0.476958\n",
      "Cost: 0.525983\n",
      "Cost: 0.371887\n",
      "Cost: 0.416254\n",
      "Cost: 0.426069\n",
      "Cost: 0.370191\n",
      "Cost: 0.517109\n",
      "Cost: 0.447613\n",
      "Cost: 0.480119\n",
      "Cost: 0.365447\n",
      "Cost: 0.424025\n",
      "Cost: 0.545569\n",
      "Cost: 0.45831\n",
      "Cost: 0.425721\n",
      "Cost: 0.451191\n",
      "Cost: 0.333398\n",
      "Cost: 0.485608\n",
      "Cost: 0.397589\n",
      "Cost: 0.584695\n",
      "Cost: 0.584031\n",
      "Cost: 0.481291\n",
      "Cost: 0.446144\n",
      "Cost: 0.533236\n",
      "Cost: 0.363198\n",
      "Cost: 0.440706\n",
      "Cost: 0.439348\n",
      "Cost: 0.50328\n",
      "Cost: 0.580831\n",
      "Cost: 0.557482\n",
      "Cost: 0.4005\n",
      "Cost: 0.540189\n",
      "Cost: 0.471552\n",
      "Cost: 0.444261\n",
      "Cost: 0.526783\n",
      "Cost: 0.402954\n",
      "Cost: 0.507689\n",
      "Cost: 0.426176\n",
      "Cost: 0.459486\n",
      "Cost: 0.492055\n",
      "Cost: 0.314699\n",
      "Cost: 0.362223\n",
      "Cost: 0.454158\n",
      "Cost: 0.478787\n",
      "Cost: 0.437438\n",
      "Cost: 0.418084\n",
      "Cost: 0.263172\n",
      "Cost: 0.4261\n",
      "Cost: 0.476175\n",
      "Cost: 0.370841\n",
      "Cost: 0.404726\n",
      "Cost: 0.581348\n",
      "Cost: 0.465252\n",
      "Cost: 0.645798\n",
      "Cost: 0.4208\n",
      "Cost: 0.335053\n",
      "Cost: 0.393752\n",
      "Cost: 0.296359\n",
      "Cost: 0.358044\n",
      "Cost: 0.325512\n",
      "Cost: 0.176894\n",
      "Cost: 0.331057\n",
      "Cost: 0.390028\n",
      "Cost: 0.432389\n",
      "Cost: 0.364271\n",
      "Cost: 0.438331\n",
      "Cost: 0.505914\n",
      "Cost: 0.601033\n",
      "Cost: 0.46845\n",
      "Cost: 0.332222\n",
      "Cost: 0.447921\n",
      "Cost: 0.368788\n",
      "Cost: 0.492005\n",
      "Cost: 0.525483\n",
      "Cost: 0.458276\n",
      "Cost: 0.319332\n",
      "Cost: 0.337943\n",
      "Cost: 0.513519\n",
      "Cost: 0.461786\n",
      "Cost: 0.448888\n",
      "Cost: 0.447155\n",
      "Cost: 0.349778\n",
      "Cost: 0.264156\n",
      "Cost: 0.399389\n",
      "Cost: 0.420166\n",
      "Cost: 0.44239\n",
      "Cost: 0.449559\n",
      "Cost: 0.372243\n",
      "Cost: 0.454782\n",
      "Cost: 0.38645\n",
      "Cost: 0.356991\n",
      "Cost: 0.341274\n",
      "Cost: 0.318558\n",
      "Cost: 0.526113\n",
      "Cost: 0.431221\n",
      "Cost: 0.373419\n",
      "Cost: 0.405073\n",
      "Cost: 0.515196\n",
      "Cost: 0.482621\n",
      "Cost: 0.260677\n",
      "Cost: 0.468456\n",
      "Cost: 0.469434\n",
      "Cost: 0.431762\n",
      "Cost: 0.324813\n",
      "Cost: 0.455465\n",
      "Cost: 0.364236\n",
      "Cost: 0.45142\n",
      "Cost: 0.273089\n",
      "Cost: 0.409953\n",
      "Cost: 0.405976\n",
      "Cost: 0.317406\n",
      "Cost: 0.244255\n",
      "Cost: 0.317488\n",
      "Cost: 0.297389\n",
      "Cost: 0.289425\n",
      "Cost: 0.444632\n",
      "Cost: 0.317412\n",
      "Cost: 0.372347\n",
      "Cost: 0.385192\n",
      "Cost: 0.427288\n",
      "Cost: 0.446074\n",
      "Cost: 0.499954\n",
      "Cost: 0.351745\n",
      "Cost: 0.386761\n",
      "Cost: 0.395554\n",
      "Cost: 0.420047\n",
      "Cost: 0.338532\n",
      "Cost: 0.504477\n",
      "Cost: 0.339431\n",
      "Cost: 0.455562\n",
      "Cost: 0.33194\n",
      "Cost: 0.34567\n",
      "Cost: 0.454075\n",
      "Cost: 0.373328\n",
      "Cost: 0.297487\n",
      "Cost: 0.360704\n",
      "Cost: 0.192479\n",
      "Cost: 0.3802\n",
      "Cost: 0.390749\n",
      "Cost: 0.294615\n",
      "Cost: 0.265091\n",
      "Cost: 0.311729\n",
      "Cost: 0.418821\n",
      "Cost: 0.204164\n",
      "Cost: 0.42924\n",
      "Cost: 0.254128\n",
      "Cost: 0.305319\n",
      "Cost: 0.517022\n",
      "Cost: 0.533225\n",
      "Cost: 0.344089\n",
      "Cost: 0.335849\n",
      "Cost: 0.377274\n",
      "Cost: 0.373669\n",
      "Cost: 0.238013\n",
      "Cost: 0.293786\n",
      "Cost: 0.403335\n",
      "Cost: 0.307119\n",
      "Cost: 0.409591\n",
      "Cost: 0.651106\n",
      "Cost: 0.237372\n",
      "Cost: 0.241753\n",
      "Cost: 0.469305\n",
      "Cost: 0.118289\n",
      "Cost: 0.378302\n",
      "Cost: 0.354961\n",
      "Cost: 0.440408\n",
      "Cost: 0.287648\n",
      "Cost: 0.383134\n",
      "Cost: 0.321725\n",
      "Cost: 0.437711\n",
      "Cost: 0.360054\n",
      "Cost: 0.27585\n",
      "Cost: 0.263207\n",
      "Testing Result.....\n",
      "Accuracy: 0.842158\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "epochs = 50\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:0.5}\n",
    "            sess.run(optimizer, feed_dict=feed_dict)\n",
    "            if step%100 == 0:\n",
    "                #每50次计算准确度\n",
    "                result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', result)   \n",
    "    print('Testing Result.....')       \n",
    "    batch_data, batch_labels = test_set.next()\n",
    "    batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "    feed_dict = {x : batch_data, y_ : batch_labels, keep_prob:1}\n",
    "    result = sess.run(accuracy, feed_dict)\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
