{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构\n",
    "\n",
    "参看VGG网络结构，适当减少了参数尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "TOWER_NAME = 'tower'\n",
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "           # with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define dog cat classification model\n",
    "class imageClassification:\n",
    "    '''Define a basic model for image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, image_holder, label_holder, keep_prob, batch_size=32, num_class=1):\n",
    "        self.image_holder = image_holder\n",
    "        self.label_holder = label_holder\n",
    "        self.num_class = num_class\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimizer\n",
    "        self.accuracy\n",
    "        self.correct_num\n",
    "        print('Initializing Image Classification Model!') \n",
    "    \n",
    "    @define_scope\n",
    "    def sigmoid(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        logits = self.inference\n",
    "        return tf.nn.sigmoid(logits)\n",
    "        \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        '''Calculate final softmax logits of \n",
    "        convolutional neural network'''\n",
    "        sigmoid = self.sigmoid\n",
    "        predictions = sigmoid > 0.5\n",
    "        return tf.cast(predictions, tf.int32)\n",
    "    \n",
    "    @define_scope\n",
    "    def cost(self):\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        labels = tf.cast(self.label_holder, tf.float32)\n",
    "        #Cross entropy\n",
    "        #cross_entropy = -tf.reduce_mean(labels*\n",
    "                                       #tf.log(self.prediction), name='cross_entropy')\n",
    "        cross_entropy = -tf.reduce_mean(labels*tf.log(self.sigmoid) + (1-labels)*tf.log(1-self.sigmoid))\n",
    "        \n",
    "        #Regularization\n",
    "        l2_loss = cross_entropy\n",
    "        for i in range(len(self.weights)):\n",
    "                l2_loss += tf.nn.l2_loss(self.weights[i])\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    @define_scope\n",
    "    def optimizer(self):\n",
    "        '''Define cross entropy loss function and regularization'''      \n",
    "        l2_loss = self.cost\n",
    "        #Decaying Learning Rate\n",
    "        cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.008\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 1000, 0.96, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(l2_loss, global_step=cur_step)\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        '''Calculate accuracy for each epoch of training'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        correct_prediction = tf.equal(self.label_holder, \n",
    "                                      self.prediction)\n",
    "        correct_ones = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "        return correct_ones\n",
    "    \n",
    "    @define_scope\n",
    "    def inference(self):\n",
    "        \"\"\"Build the clasification model.\n",
    "        Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "        Returns:\n",
    "        Logits.\n",
    "        \"\"\"\n",
    "        # We instantiate all variables using tf.get_variable() instead of\n",
    "        # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "        # If we only ran this model on a single GPU, we could simplify this function\n",
    "        # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "        #\n",
    "        # conv1\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=[3, 3, 3, 64],\n",
    "                                                 wd=0.0)\n",
    "            conv = tf.nn.conv2d(self.image_holder, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv1)\n",
    "        # pool1\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "        \n",
    "        # conv2\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=[3, 3, 64, 128],                                                 \n",
    "                                                 wd=0.0)\n",
    "            conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [128], tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv2)      \n",
    "\n",
    "        # pool2\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "        \n",
    "        # conv3\n",
    "        with tf.variable_scope('conv3') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                                                 shape=[3, 3, 128, 256],                                                 \n",
    "                                                 wd=0.0)\n",
    "            conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [256], tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv3 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv2)      \n",
    "\n",
    "        # pool2\n",
    "        pool3 = tf.nn.max_pool(conv3, ksize=[1, 3, 3, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "        \n",
    "        pool3_dropout = tf.nn.dropout(pool3, self.keep_prob)\n",
    "        # local3\n",
    "        with tf.variable_scope('fully_connected1') as scope:\n",
    "            # Move everything into depth so we can perform a single matrix multiply.\n",
    "            reshape_tensor = tf.reshape(pool3_dropout, [self.batch_size, -1], name='reshape_tensor')\n",
    "            dim = reshape_tensor.get_shape()[1].value\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                                  wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape_tensor, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local3)\n",
    "            \n",
    "        local3_dropout = tf.nn.dropout(local3, self.keep_prob)\n",
    "        # local4\n",
    "        with tf.variable_scope('fully_connected2') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                                  wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "            local4 = tf.nn.relu(tf.matmul(local3_dropout, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local4)\n",
    "            \n",
    "        local4_dropout = tf.nn.dropout(local4, self.keep_prob)\n",
    "        \n",
    "        # linear layer(WX + b),\n",
    "        # We don't apply softmax here because\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "        with tf.variable_scope('softmax_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [192, self.num_class],\n",
    "                                                  wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_class],\n",
    "                                      tf.constant_initializer(0.0))\n",
    "            softmax_linear = tf.add(tf.matmul(local4_dropout, weights), biases, name=scope.name)\n",
    "            self._activation_summary(softmax_linear)\n",
    "        return softmax_linear\n",
    "    \n",
    "    #def _conv_layer(self, )\n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        \"\"\"Helper to create summaries for activations.\n",
    "        Creates a summary that provides a histogram of activations.\n",
    "        Creates a summary that measures the sparsity of activations.\n",
    "        Args:\n",
    "        x: Tensor\n",
    "        Returns:\n",
    "        nothing\n",
    "        \"\"\"\n",
    "        # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "        # session. This helps the clarity of presentation on tensorboard.\n",
    "        tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                          tf.nn.zero_fraction(x))\n",
    "    \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope('weights'):\n",
    "                dtype = tf.float32\n",
    "                var = tf.get_variable(initializer=initializer(shape), dtype=dtype, name=name)\n",
    "            #var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _variable_with_weight_decay(self, name, shape, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "        Returns:\n",
    "        Variable Tensor\n",
    "        \"\"\"\n",
    "        dtype = tf.float32\n",
    "        var = self._variable_on_cpu(name, shape,\n",
    "                               tf.contrib.layers.xavier_initializer())\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            #tf.add_to_collection('losses', weight_decay)\n",
    "            self.weights.append(weight_decay)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义输入张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Image Classification Model!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    model = imageClassification(image_holder, label_holder, keep_prob, batch_size, num_class =1)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "path = 'C:/Users/onlooker/Documents/deeplearning_projects/cats_dogs_classification/dataset'\n",
    "training_set = train_datagen.flow_from_directory(path+'/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(path+'/test_set', shuffle=False,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.94332\n",
      "Cost: 1.81627\n",
      "Cost: 1.79601\n",
      "Cost: 1.6962\n",
      "Cost: 1.69042\n",
      "Cost: 1.64443\n",
      "Cost: 1.55749\n",
      "Cost: 1.49317\n",
      "Cost: 1.43715\n",
      "Cost: 1.38808\n",
      "Cost: 1.32744\n",
      "Cost: 1.27909\n",
      "Cost: 1.38868\n",
      "Cost: 1.25888\n",
      "Cost: 1.25231\n",
      "Cost: 1.30282\n",
      "Cost: 1.26781\n",
      "Cost: 1.19784\n",
      "Cost: 1.2011\n",
      "Cost: 1.07712\n",
      "Cost: 0.98704\n",
      "Cost: 1.13203\n",
      "Cost: 1.09172\n",
      "Cost: 1.04107\n",
      "Cost: 0.969746\n",
      "Cost: 1.00542\n",
      "Cost: 0.923957\n",
      "Cost: 0.975079\n",
      "Cost: 0.95894\n",
      "Cost: 0.913933\n",
      "Cost: 0.878574\n",
      "Cost: 0.988095\n",
      "Cost: 0.924557\n",
      "Cost: 0.850962\n",
      "Cost: 0.804787\n",
      "Cost: 0.918714\n",
      "Cost: 0.700105\n",
      "Cost: 0.923358\n",
      "Cost: 0.852657\n",
      "Cost: 0.871645\n",
      "Cost: 0.772391\n",
      "Cost: 0.721123\n",
      "Cost: 0.860832\n",
      "Cost: 0.676857\n",
      "Cost: 0.763931\n",
      "Cost: 0.702629\n",
      "Cost: 0.759242\n",
      "Cost: 0.725119\n",
      "Cost: 0.761793\n",
      "Cost: 0.75006\n",
      "Cost: 0.82626\n",
      "Cost: 0.574702\n",
      "Cost: 0.631242\n",
      "Cost: 0.660351\n",
      "Cost: 0.714003\n",
      "Cost: 0.665653\n",
      "Cost: 0.722263\n",
      "Cost: 0.719482\n",
      "Cost: 0.741925\n",
      "Cost: 0.723552\n",
      "Cost: 0.618318\n",
      "Cost: 0.653924\n",
      "Cost: 0.724325\n",
      "Cost: 0.696895\n",
      "Cost: 0.578147\n",
      "Cost: 0.501305\n",
      "Cost: 0.497982\n",
      "Cost: 0.478435\n",
      "Cost: 0.698608\n",
      "Cost: 0.540453\n",
      "Cost: 0.650458\n",
      "Cost: 0.621957\n",
      "Cost: 0.591159\n",
      "Cost: 0.452711\n",
      "Cost: 0.460705\n",
      "Cost: 0.485059\n",
      "Cost: 0.640268\n",
      "Cost: 0.55092\n",
      "Cost: 0.549448\n",
      "Cost: 0.531186\n",
      "Cost: 0.643633\n",
      "Cost: 0.487147\n",
      "Cost: 0.456435\n",
      "Cost: 0.551118\n",
      "Cost: 0.649121\n",
      "Cost: 0.705816\n",
      "Cost: 0.507451\n",
      "Cost: 0.633089\n",
      "Cost: 0.477037\n",
      "Cost: 0.454179\n",
      "Cost: 0.491147\n",
      "Cost: 0.423883\n",
      "Cost: 0.522103\n",
      "Cost: 0.446499\n",
      "Cost: 0.429398\n",
      "Cost: 0.533876\n",
      "Cost: 0.581972\n",
      "Cost: 0.547868\n",
      "Cost: 0.563764\n",
      "Cost: 0.61328\n",
      "Cost: 0.678683\n",
      "Cost: 0.403145\n",
      "Cost: 0.478991\n",
      "Cost: 0.50478\n",
      "Cost: 0.398133\n",
      "Cost: 0.39991\n",
      "Cost: 0.471729\n",
      "Cost: 0.50292\n",
      "Cost: 0.405172\n",
      "Cost: 0.515896\n",
      "Cost: 0.421413\n",
      "Cost: 0.512345\n",
      "Cost: 0.468776\n",
      "Cost: 0.554779\n",
      "Cost: 0.583842\n",
      "Cost: 0.522345\n",
      "Cost: 0.384827\n",
      "Cost: 0.414226\n",
      "Cost: 0.440228\n",
      "Cost: 0.490452\n",
      "Cost: 0.549267\n",
      "Cost: 0.384815\n",
      "Cost: 0.350124\n",
      "Cost: 0.389824\n",
      "Cost: 0.494012\n",
      "Cost: 0.522976\n",
      "Cost: 0.573979\n",
      "Cost: 0.468967\n",
      "Cost: 0.396953\n",
      "Cost: 0.303318\n",
      "Cost: 0.353407\n",
      "Cost: 0.498989\n",
      "Cost: 0.37127\n",
      "Cost: 0.458815\n",
      "Cost: 0.447082\n",
      "Cost: 0.369267\n",
      "Cost: 0.354662\n",
      "Cost: 0.443694\n",
      "Cost: 0.396405\n",
      "Cost: 0.444395\n",
      "Cost: 0.382889\n",
      "Cost: 0.379028\n",
      "Cost: 0.321235\n",
      "Cost: 0.351601\n",
      "Cost: 0.331364\n",
      "Cost: 0.362534\n",
      "Cost: 0.428016\n",
      "Cost: 0.433486\n",
      "Cost: 0.351431\n",
      "Cost: 0.435912\n",
      "Cost: 0.445934\n",
      "Cost: 0.436959\n",
      "Cost: 0.377937\n",
      "Cost: 0.455479\n",
      "Cost: 0.342795\n",
      "Cost: 0.368824\n",
      "Cost: 0.302133\n",
      "Cost: 0.302916\n",
      "Cost: 0.454782\n",
      "Cost: 0.422056\n",
      "Cost: 0.323481\n",
      "Cost: 0.440816\n",
      "Cost: 0.373883\n",
      "Cost: 0.300001\n",
      "Cost: 0.408443\n",
      "Cost: 0.362013\n",
      "Cost: 0.426537\n",
      "Cost: 0.302121\n",
      "Cost: 0.274742\n",
      "Cost: 0.425282\n",
      "Cost: 0.313476\n",
      "Cost: 0.30969\n",
      "Cost: 0.34263\n",
      "Cost: 0.345799\n",
      "Cost: 0.369348\n",
      "Cost: 0.339872\n",
      "Cost: 0.351988\n",
      "Cost: 0.304863\n",
      "Cost: 0.294585\n",
      "Cost: 0.271591\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "epochs = 30\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            #产生训练用样本集      \n",
    "            batch_data, batch_labels = training_set.next()\n",
    "            batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "            #数据传递给tensorflow\n",
    "            feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 0.5}\n",
    "            _, loss = sess.run([model.optimizer, model.cost], feed_dict=feed_dict)\n",
    "            if step%400 == 0:\n",
    "                #每50次计算准确度\n",
    "                #result = sess.run(cross_entropy, feed_dict={x : batch_data, y_ : batch_labels, keep_prob:1})\n",
    "                print('Cost:', loss) \n",
    "                #保存模型\n",
    "        saver.save(sess, \"Model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n",
      "Testing Result.....\n",
      "Accuracy: 0.8984375\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"./Model/model.ckpt\") # 注意此处路径前添加\"./\" \n",
    "    print('Testing Result.....')  \n",
    "    count = 0\n",
    "    for i in range(30):\n",
    "        batch_data, batch_labels = test_set.next()\n",
    "        #print(batch_data.shape)\n",
    "        #print(i)\n",
    "        batch_labels = np.reshape(batch_labels, [-1,1])\n",
    "        feed_dict = {image_holder: batch_data, label_holder : batch_labels, keep_prob: 1}\n",
    "        c = sess.run(model.correct_num, feed_dict=feed_dict)\n",
    "        count += c\n",
    "    result = count*1.0/(30*64)\n",
    "    print('Accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
